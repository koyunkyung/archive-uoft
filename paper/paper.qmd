---
title: "A Poll-of-Polls Forecast for the 2024 U.S. Presidential Election: Analyzing Support Trends for Harris by Pollster and State"
subtitle: "Kamala Harris Leads with Stable Support Around 48%"
author: 
  - Yunkyung Ko
thanks: "Code and data are available at: [https://github.com/koyunkyung/us_election_2024](https://github.com/koyunkyung/us_election_2024)."
date: today
date-format: long
abstract: "In the anticipation of the 2024 U.S. presidential election, this paper analyzes trends in Harris's support, focusing on pollster and state-specific factors. Harrisâ€™s support has remained steady around 48%, though key pollsters like Siena/NYT and battleground states such as Michigan show notable deviations. Our goal is to propose a reliable electoral prediction by examining correlations among date, pollster, and state variables and developing predictions through linear and Bayesian models."
format: pdf
number-sections: true
bibliography: references.bib
toc: true
toc-title: "Table of Contents"
toc-depth: 2
toc-location: left
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(readr)
library(janitor)
library(lubridate)
library(broom)
library(modelsummary)
library(rstanarm)
library(splines)
library(ggplot2)
library(knitr)
library(kableExtra)
library(gridExtra)
library(here)
library(usmap)  # For the map of US states
library(viridis)  # For a color palette

raw_elections_data <- read_csv(here::here("data/01-raw_data/raw_elections_data.csv"))
harris_data <- read_csv(here::here("data/02-analysis_data/harris_elections_data.csv"))
```


# Introduction

The 2024 U.S. presidential election receives a lot of international attention due to its far-reaching implications, affecting global economy, foreign policy, and social issues like climate change [@euromonitor2024]. This paper aims to predict possible outcomes of the election by analyzing Kamala Harris's support in the polling data with a linear regression and a Bayesian approach.

Our estimand is Harris's support rate in polls, tracked over time and adjusted for pollster and state specific variations. Using a "pooling the polls" approach, we aim to correct for variations across different voter bases [@poolingpolls]. Initial linear models show Harris's support rate remains stable at around 48% over time, though with large variability as seen in the spread of points around the fitted line. Some pollsters or states such as Siena/NYT and Michigan consistently reported higher or lower support for Harris compared to others, proposing the importance of accounting for specific polling contexts.

Beyond election forecasting, this analysis signals global stakeholders on potential shifts in the U.S. policy and strategy [@csis2024]. As such, this study not only contributes to the domestic political discourse but also provides a valuable tool for global actors seeking to navigate the uncertainty surrounding the 2024 U.S. presidential election [@csis2024]. The paper is organized as follows: @sec-data and @sec-model detail the data and methodology used, including filtering and modeling techniques applied to the polling data. @sec-results presents findings from the linear and Bayesian models, while @sec-discussion interprets broader implications. Finally, @sec-appendix covers pollster methodology and ideal survey practices.


# Data {#sec-data}

## Overview

We use the statistical programming language R [@citeR] to analyze US presidential polling data from FiveThirtyEight [@fivethirtyeight2024], focusing on support for Kamala Harris. The dataset, last updated on 30 October 2024, includes a wide range of poll results from various national and state-level polls, with key variables such as pollster, sample size, percentage of support for Harris, and end date of the poll. Following the guidance of @tellingstories, we compiled the results of each opinion poll over a period of time and compared them taking into account the methodological peculiarities of polling by pollsters and geographical scope of the conducted polls. 

To ensure data quality, we filtered the dataset to include only polls that measured Kamala Harris' support, with a numeric grade of pollster 2.7 or higher for reliability. We also limited the analysis to polls conducted after July 21, 2024, when Harris officially declared her candidacy, and excluded pollsters with fewer than 30 polls to focus on those with sufficient data for robust results.

In performing the analysis, we utilized several R packages. `tidyverse` [@tidyverse] was used for data manipulation and visualization and `rstanarm` [@rstanarm], `modelsummary` [@modelsummary] was respectively used for Bayesian modeling and generating model summaries. For visualizing results, `ggplot2` [@ggplot2] was used and `kableExtra` [@kableExtra] helped format tables for presentation. These packages provided a framework for efficient data processing, modeling, and reporting.


## Measurement


The original dataset sourced from FiveThirtyEight [@fivethirtyeight2024] aggregates a wide range of poll results, showing 16817 observations based on dataset available at October 30. The polls conducted by various polling organizations capture voter preferences by taking a representative sample of the electorate and asking for the voters' candidate of choice. Surveys were conducted at the state and national levels, providing the wide perspective on public feelings across the country.

Each poll represents a predictor of an actual event, namely voter opinion at a particular moment. Nevertheless, like all survey results, the raw data is susceptible to many potential limitations including the following: sampling error, variation in polling methods, distortion because of inappropriate survey responses such as missing data or response from respondents who misunderstood the questions [@tellingstories].

While applying several filters to the original dataset such as restricting to those with a numeric grade of 2.7 or higher or pollsters with more than 30 polls improves data reliability, certain limitations still exist. Selection bias and sampling error remains as a concern, since polls always represent only part of the population. Differences in the way different organizations conducted their polling might introduce more inconsistencies. Finally, by focusing our attention on post-declaration polls only, we exclude earlier trends that could add more insight into how Harris' support has evolved over time. 

## Outcome variable

### The Proportion (%) of Support that a Candidate Received in the Poll 

The main variable of interest that we aim to forecast is the 'pct' variable, which represents the proportion of vote or support that a candidate received in the poll. @tbl-pctraw1 and @fig-pct2 respectively shows the summary statistics and distribution of the 'pct' variable in the original dataset [@fivethirtyeight2024]. @tbl-pctfilter1 and @fig-pctfilter2 shows the summary statistics and distribution of the same variable, but in the filtered dataset that only comprises of the supporting votes for Harris from relatively high-quality polling organizations. Comparing the summary statistics for the raw data (@tbl-pctraw1) and filtered data (@tbl-pctfilter1), higher numbers were derived from data filtered only by Harris supporters. Also, @fig-pctfilter2 illustrates that a significant number of polls indicate support levels ranging from 45% to 50%, which suggests a stable yet not substantial endorsement. This proposes that Harris possesses a reliable foundational support, although her capacity to obtain a majority remains ambiguous.

\newpage

```{r}
#| label: tbl-pctraw1
#| tbl-cap: Summary statistics for the proportion (%) of support that candidates received in the poll
#| echo: false
#| tbl-align: center
#| tbl-width: 80%


# Calculate summary statistics for the 'pct' variable
summary_stats <- raw_elections_data %>%
  summarize(
    mean = round(mean(pct, na.rm = TRUE), 2),
    median = round(median(pct, na.rm = TRUE),2),
    min = round(min(pct, na.rm = TRUE), 2),
    max = round(max(pct, na.rm = TRUE), 2),
    sd = round(sd(pct, na.rm = TRUE), 2),
    n = n()
  )

# Display the summary statistics in a nicely formatted table
summary_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```
```{r}
#| label: fig-pct2
#| fig-cap: Distribution of the proportion (%) of support that candidates received in the poll
#| echo: false
#| fig-align: center
#| fig-width: 5
#| page-break-after: true


# Plot the distribution of 'pct' using a histogram
ggplot(raw_elections_data, aes(x = pct)) +
  geom_histogram(binwidth = 3, fill = "blue", alpha = 0.7, color = "black") +
  theme_minimal() +
  labs(x = "Percentage of Support (pct)",
       y = "Count")

```
```{r}
#| label: tbl-pctfilter1
#| tbl-cap: Summary statistics for the proportion (%) of support that Harris received in high-quality polls
#| echo: false
#| tbl-align: center
#| tbl-width: 80%


# Calculate summary statistics for the 'pct' variable
summary_stats <- harris_data %>%
  summarize(
    mean = round(mean(pct, na.rm = TRUE), 2),
    median = round(median(pct, na.rm = TRUE), 2),
    min = round(min(pct, na.rm = TRUE), 2),
    max = round(max(pct, na.rm = TRUE), 2),
    sd = round(sd(pct, na.rm = TRUE), 2),
    n = n()
  )

# Display the summary statistics in a nicely formatted table
summary_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```

\newpage

```{r}
#| label: fig-pctfilter2
#| fig-cap: Distribution of the proportion (%) of support that Harris received in high-quality polls
#| echo: false
#| fig-align: center
#| fig-width: 5
#| page-break-after: true

# Plot the distribution of 'pct' using a histogram
ggplot(harris_data, aes(x = pct)) +
  geom_histogram(binwidth = 2, fill = "blue", alpha = 0.7, color = "black") +
  theme_minimal() +
  labs(x = "Percentage of Support (pct)",
       y = "Count")

```


## Predictor variables

### The Date the Poll was Concluded

The 'end_date' variable representing the time the poll was concluded was put into account to keep track of how support for a candidate changes in time. The reported end dates in the original dataset [@fivethirtyeight2024] ranges from 7 April, 2021 to 29 October, 2024 (@tbl-date). @fig-date shows that the polling data is more concentrated on survey results conducted in the recent period.

```{r}
#| label: tbl-date
#| tbl-cap: Summary statistics for the date that the poll was concluded
#| echo: false
#| tbl-align: center
#| tbl-width: 80%


# Convert 'end_date' to a proper date format
raw_elections_dataa <- raw_elections_data %>%
  mutate(end_date = mdy(end_date))
filtered_data <- raw_elections_dataa %>%
  filter(end_date >= as.Date("2023-01-01"))

summary_stats <- raw_elections_dataa %>%
  summarize(
    Min = min(end_date, na.rm = TRUE),
    Max = max(end_date, na.rm = TRUE)
  )

# Display the summary statistics in a nicely formatted table
summary_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```

\newpage

```{r}
#| label: fig-date
#| fig-cap: Distribution of the date that the poll was concluded
#| echo: false
#| fig-align: center
#| fig-width: 5
#| page-break-after: true

# Distribution of 'end_date'
ggplot(filtered_data, aes(x = end_date)) +
  geom_histogram(fill = "grey", alpha = 0.7, color = "black", binwidth = 30) +  # Set binwidth explicitly
  theme_minimal() +
  labs(x = "End Date",
       y = "Count")

```


When filtering the data, not only pollster quality and candidate type but also the date variable was considered. The filtered data contains only the polling data after the declaration of Harris. So, the date variable for filtered data in @tbl-date2 ranges from 23 July, 2024 to 29 October, 2024. @fig-date2 shows that overall, polling is conducted regularly but intensifies around specific dates.

```{r}
#| label: tbl-date2
#| tbl-cap: Summary statistics for the date that high-quality polls for Harris was concluded
#| echo: false
#| tbl-align: center
#| tbl-width: 80%


summary_stats <- harris_data %>%
  summarize(
    Min = min(end_date, na.rm = TRUE),
    Max = max(end_date, na.rm = TRUE)
  )

# Display the summary statistics in a nicely formatted table
summary_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```

\newpage

```{r}
#| label: fig-date2
#| fig-cap: Distribution of the date that high-quality polls for Harris was concluded
#| echo: false
#| fig-align: center
#| fig-width: 5
#| page-break-after: true

# Convert 'end_date' to a proper date format
harris_dataa <- harris_data %>%
  mutate(end_date = ymd(end_date))  # Change mdy() to ymd() or dmy() depending on the format

# Distribution of 'end_date'
ggplot(harris_dataa, aes(x = end_date)) +
  geom_histogram(fill = "grey", alpha = 0.7, color = "black", binwidth = 10) +  # Set binwidth explicitly
  theme_minimal() +
  labs(x = "End Date",
       y = "Count")

```

### Pollster and State

The 'pollster' and 'state' variable were selected to consider the effect of changes in poll-making organizations and geographical distinctions. The two variables respectively represent the polling organization that conducted the poll and the US state where the poll was conducted or focused.

@tbl-predictors1 shows that the original dataset [@fivethirtyeight2024] contains 232 distinct pollsters and 56 distinct states. After filtering for high-quality polls and assigning 'other' for states with fewer than 60 polls, the analysis data contains 8 distinct poll-making organizations and 20 geographical distinctions as shown in @tbl-predictors2.

```{r}
#| label: tbl-predictors1
#| tbl-cap: Number of distinct polling organizations and US states where the poll was conducted
#| echo: false
#| tbl-align: center
#| tbl-width: 80%

summary_stats <- raw_elections_data %>%
  summarize(
    Pollster = n_distinct(pollster),
    State = n_distinct(state)
  )

# Display the summary statistics in a nicely formatted table
summary_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```

```{r}
#| label: tbl-predictors2
#| tbl-cap: Number of distinct high-quality polling organizations and US states where more than 60 polls for Harris were conducted
#| echo: false
#| tbl-align: center
#| tbl-width: 80%


summary_stats <- harris_data %>%
  summarize(
    Pollster = n_distinct(pollster),
    State = n_distinct(state)
  )

# Display the summary statistics in a nicely formatted table
summary_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```

The distribution of polling counts for different pollsters in @fig-pollster suggests that the analysis data is dominated by a few pollsters, particularly Siena/NYT. Depending on their polling methodology, the general results may have potential biases. A detailed analysis of the polling methodology and possible errors of the organization will be covered in @sec-appendix.

```{r}
#| label: fig-pollster
#| fig-cap: Distribution of polling organizations where high-quality polls for Harris were conducted
#| echo: false
#| fig-align: center
#| fig-width: 5
#| page-break-after: true


# Distribution of 'pollster' (bar plot since it's categorical)
ggplot(harris_data, aes(x = pollster)) +
  geom_bar(fill = "purple", alpha = 0.7, color = "black") +
  theme_minimal() +
  labs(x = "Pollster",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

@fig-state displays the distribution of polls across different states in the analysis data. Pennsylvania, Arizona, and Georgia are the top 3 states with high number of polls while states like Montana, New Mexico, and Maryland have much fewer polls. Note that a significant number of national or unspecified state-level polls are aggregated in this analysis data regarding the high count in 'Other' category. The concentration of polls in certain states further suggests a strategic focus on areas likely to impact the election outcome [@11alive2024].

```{r}
#| label: fig-state
#| fig-cap: Distribution of US states where the more than 60 polls for Harris were conducted
#| echo: false
#| fig-align: center
#| fig-width: 5
#| page-break-after: true


# Distribution of 'state' (bar plot)
ggplot(harris_data, aes(x = reorder(state, -table(state)[state]))) +
  geom_bar(fill = "black", alpha = 0.7, color = "black") +
  theme_minimal() +
  labs(x = "State",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

### Pollscore Measuring the Validity of Polling Questions 

This variable was another factor we had put into consideration to check whether the validity of the polling questions affects the polling results. The 'pollscore' variable represents the score or reliability of the pollster in question. The numeric values are the error and bias that can be attributed to the pollster, which means negative numbers are better. @tbl-pollscore and @fig-pollscore suggests that while the majority of the polls are moderately to highly qualitative in the original dataset, a fraction of the polls with low-quality or no scores could add noise or uncertainty to the analysis.

```{r}
#| label: tbl-pollscore
#| tbl-cap: Summary statistics for the reliability scores of pollsters
#| echo: false
#| tbl-align: center
#| tbl-width: 80%


summary_stats <- raw_elections_data %>%
  summarize(
    mean = round(mean(pollscore, na.rm = TRUE),2),
    median = round(median(pollscore, na.rm = TRUE),2),
    min = round(min(pollscore, na.rm = TRUE),2),
    max = round(max(pollscore, na.rm = TRUE),2),
    sd = round(sd(pollscore, na.rm = TRUE),2),
    n = n()
  )

# Display the summary statistics in a nicely formatted table
summary_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```

\newpage

```{r}
#| label: fig-pollscore
#| fig-cap: Distribution of the reliability scores of pollsters
#| echo: false
#| fig-align: center
#| fig-width: 5
#| page-break-after: true

# Distribution of 'pollscore' (bar plot) with simplified x-axis labels
ggplot(raw_elections_data, aes(x = factor(pollscore))) +
  geom_bar(fill = "skyblue", alpha = 0.7, color = "black") +
  theme_minimal() +
  scale_x_discrete(breaks = c(-1.5, -1, -0.5, 0, 0.5, 1, 1.5)) +  # Simplified labels at 0.5 intervals
  labs(x = "Pollscore",
       y = "Count")


```

After the filtering to polling data of high-quality polling organizations, we can find that the overall value and standard deviation of pollscores went down in @tbl-pollscore2. This implies that the polling data narrowed down to the responses from more reliable survey questions. @fig-pollscore2 also indicates that the data cleaning process effectively excluded less reliable sources, which can enhance the robustness of subsequent analyses.

```{r}
#| label: tbl-pollscore2
#| tbl-cap: Summary statistics for the reliability scores of high-quality pollsters in the analysis data
#| echo: false
#| tbl-align: center
#| tbl-width: 80%


summary_stats <- harris_data %>%
  summarize(
    mean = round(mean(pollscore, na.rm = TRUE),2),
    median = round(median(pollscore, na.rm = TRUE),2),
    min = round(min(pollscore, na.rm = TRUE),2),
    max = round(max(pollscore, na.rm = TRUE),2),
    sd = round(sd(pollscore, na.rm = TRUE),2),
    n = n()
  )

# Display the summary statistics in a nicely formatted table
summary_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```

\newpage

```{r}
#| label: fig-pollscore2
#| fig-cap: Distribution of the reliability scores of high-quality pollsters in the analysis data
#| echo: false
#| fig-align: center
#| fig-width: 5
#| page-break-after: true

# Distribution of 'pollscore' (bar plot)
ggplot(harris_data, aes(x = factor(pollscore))) +
  geom_bar(fill = "skyblue", alpha = 0.7, color = "black", width = 0.5) +
  theme_minimal() +
  labs(x = "Pollscore",
       y = "Count")

```

## Correlation between predictor variables

By examining the relationships between predictor variables in the analysis dataset, we aim to identify potential biases in interpretation and model instability. This is expected to ensure a transparent understanding of the model's robustness and interpretability.

### End Date and State

@fig-correlation1 shows notable polling concentrations on certain states such as Pennsylvania, Ohio, and Nevada in week 2024-37 (September 9, 2024) or 2024-38 (September 15, 2024). This suggests that these states are key battelgrounds or areas of strategic focus during the election period. Moreover, polling activities are not consistent across all weeks and there are clear peaks in polling activity in week 2024-38 (September 15, 2024), which may correspond to significant political events, debates, or media focuses. 

\newpage

```{r}
#| label: fig-correlation1
#| fig-cap: "Polling concentration over time by state (*Note:* Strategic focus of polling is directed in battleground states such as Pennsylvania and Ohio in September 15, 2024)"
#| echo: false
#| fig-align: center
#| page-break-after: true


# Aggregate the number of polls by state and by week (or month)
poll_time_state_2 <- harris_data %>%
  mutate(week = format(as.Date(end_date), "%Y-%U")) %>%  # Group by week of the year
  group_by(state, week) %>%
  summarise(total_polls = n(), .groups = "drop")  # Adding .groups = "drop"


# Plot a heatmap of polling activity by state and week
ggplot(poll_time_state_2, aes(x = week, y = state, fill = total_polls)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue", na.value = "grey90") +
  labs(x = "Week",
       y = "State",
       fill = "Total Polls") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), strip.text = element_text(size = 8))

```


### End Date and Pollster

@fig-correlation2 shows that Siena/NYT, which made up the largest proportion of polls in our analysis dataset, has concentrated polling efforts in week 2024-37 (September 9, 2024) and week 2024-38 (September 15, 2024). Notably active periods, possibly around key election events, might introduce temporal bias and overemphasize methodologies of Siena/NYT in that specific time period. Emerson shows overall consistency of polling activities across several weeks indicating steady involvement, while other polling organizations show more sporadic or minimal involvement.

\newpage

```{r}
#| label: fig-correlation2
#| fig-cap: "Polling concentration over time by pollster (*Note:* Concentrated polling efforts of Siena/NYT are shown in the first half of September, 2024.)"
#| echo: false
#| fig-align: center
#| page-break-after: true


# Aggregate the number of polls by state and by week (or month)
poll_time_pollster <- harris_data %>%
  mutate(week = format(as.Date(end_date), "%Y-%U")) %>%  # Group by week of the year
  group_by(pollster, week) %>%
  summarise(total_polls = n(), .groups = "drop")  # Adding .groups = "drop"


# Plot a heatmap of polling activity by state and week
ggplot(poll_time_pollster, aes(x = week, y = pollster, fill = total_polls)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue", na.value = "grey90") +
  labs(x = "Week",
       y = "Pollster",
       fill = "Total Polls") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), strip.text = element_text(size = 8))

```


### Pollster and State

@fig-correlation3 shows that Siena/NYT has the strongest state-level presence in key battleground states like Michigan and Arizona. Others like AtlasIntel and Beacon/Shaw has minimal polling coverage in targeted states such as Nevada and Georgia. This variation in coverage could influence the overall analysis in that some states might receive disproportionately more attention from certain pollsters.

\newpage

```{r}
#| label: fig-correlation3
#| fig-cap: "Polling activity by pollster and state (*Note:* Siena/NYT has strongest state-level polling coverage in key battleground states)"
#| echo: false
#| page-break-after: true


# Step 1: Filter out NAs from pollster column and summarize data by state and pollster
pollster_state_data <- harris_data %>%
  filter(!is.na(pollster)) %>%  # Remove NAs from pollster
  group_by(state, pollster) %>%
  summarise(total_polls = n(), .groups = "drop")  # Summarize number of polls

# Step 2: Plot the faceted map using plot_usmap
plot_usmap(data = pollster_state_data, values = "total_polls", regions = "states") +
  facet_wrap(~ pollster, ncol = 4) +  # Create a map for each pollster
  scale_fill_distiller(palette = "Blues", direction = 1, na.value = "gray90") +  # Use blue color palette
  labs(fill = "Total Polls") +
  theme_void() +  # Remove axes and gridlines
  theme(strip.text = element_text(size = 9))  # Customize facet labels

```


# Model {#sec-model}

Our modelling strategy estimates Kamala Harris's support percentage in the 2024 US election polls, accounting for potential variations over time including pollster and state differences. The model balances complexity with interpretability, using both linear and Bayesian frameworks to capture patterns in the data. We turned from the initial linear model to a Bayesian model for robust predictions considering the variables' uncertainties. The Bayesian approach, which incorporates both prior beliefs and observed data to make predictions, allowed us to capture the variances among pollsters and states more accurately and responsively. Further background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ as the percentage of support that Kamala Harris receives in poll $i$. We begin with two simple linear models and progress to more complex Bayesian models that account for hierarchical structures.

The following models outline our approach:

### Linear Model by Date 

```{=latex}
\begin{align}  
y_i &= \beta_0 + \beta_1 \cdot \text{end\_date}_i + \epsilon_i \\
\epsilon_i &\sim \text{Normal}(0, \sigma^2)
\end{align}
```

where:
- $y_i$ is the percentage of support for Harris in poll $i$,
- $\beta_0$ is the intercept,
- $\beta_1$ represents the effect of the poll's end date,
- $\epsilon_i$ is the error term.

### Linear Model by Date and Pollster

```{=latex}
\begin{align}  
y_i &= \beta_0 + \beta_1 \cdot \text{end\_date}_i + \gamma_{p[i]} + \epsilon_i \\
\epsilon_i &\sim \text{Normal}(0, \sigma^2)
\end{align}
```

where:
- $\gamma_{p[i]}$ is a fixed effect for pollster $p$ conducting poll $i$ (e.g., Siena/NYT).

### Bayesian Model with Random Intercept for Pollster 

```{=latex}
\begin{align}  
y_i | \mu_i, \sigma &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \beta_0 + \beta_1 \cdot \text{end\_date}_i + \gamma_{p[i]} \\
\gamma_p &\sim \text{Normal}(0, \sigma_{\gamma})
\end{align}
```

where:
- $\gamma_p$ is a random effect for pollster $p$.

### Bayesian Model with Random Intercept for Pollster and State

```{=latex}
\begin{align}  
y_i | \mu_i, \sigma &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \beta_0 + \beta_1 \cdot \text{end\_date}_i + \gamma_{p[i]} + \delta_{s[i]} \\
\gamma_p &\sim \text{Normal}(0, \sigma_{\gamma}) \\
\delta_s &\sim \text{Normal}(0, \sigma_{\delta})
\end{align}
```

where:
- $\delta_{s[i]}$ is a random effect for state $s$.

The Bayesian models are fit using `rstanarm` in R. The priors used are weakly informative:
- $\beta_0 \sim \text{Normal}(0, 10)$
- $\sigma \sim \text{Exponential}(1)$

### Model justification

Different pollsters and states induce variations in polling results, as pollsters may have distinct methodologies and states represent diverse voter bases. Incorporating random effects for both pollsters and states allows us to improve the robustness of the model.

These models are run through the rstanarm package [@rstanarm] in R  [@citeR], which makes Bayesian modeling available through the use of Stan's strong inference engine. To validate the models, RMSE and WAIC have been made use of to check the goodness of fit; Bayesian models with reduced RMSE and WAIC outperform linear models. We use weakly informative priors; for example, $\beta_0 \sim \text{Normal}(0, 10)$ and $\sigma \sim \text{Exponential}(1)$. This reflects our initial uncertainty but prevents overfitting. The priors were chosen conservatively to ensure that the model remains consistent.

Model diagnostics, including posterior predictive checks and convergence diagnostics, were carried out to ensure the reliability of the results. The Bayesian models converged successfully, as indicated by $\hat{R} = 1$ for all parameters.

The main assumption in these models is that the pollster and state effects can be treated as random. This assumes that the effects are normally distributed across pollsters and states, which may not always be accurate. Additionally, the model assumes that polling data is representative of the actual electorate, an assumption that can be violated if polls are biased or have non-random sampling issues. Despite these limitations, the hierarchical structure allows us to capture important variability, making the model suitable for predicting Harris's support. Future improvements could involve incorporating time-varying effects or exploring interactions between pollsters and states.

# Results {#sec-results}

## Results from examining the analysis data

@fig-resultplot1 shows an initial increase in support for Harris since her declaration to join the electoral race, peaking around mid-September. However, this initial momentum stabilized with minor decrease in support as the election gets closer. Moreover, the scattered data points around the summary line show high variance between individual polls.

\newpage

```{r}
#| label: fig-resultplot1
#| fig-cap: "Polling votes for Harris over time (*Note:* The date starts from after Harris officially announced her campaign for 2024 U.S. presidential election on July 21, 2024.)"
#| echo: false
#| warning: false
#| fig-align: center
#| fig-width: 5
#| page-break-after: true


base_plot <- ggplot(harris_data, aes(x = end_date, y = pct)) +
  theme_classic() +
  labs(y = "Harris percent", x = "Date")

# Plots poll estimates and overall smoothing
base_plot +
  geom_point() +
  geom_smooth()
```

When accounting for the pollster difference, @fig-resultplot2 and @fig-resultplot3 show variations in Harris' support levels that are obscured in the aggregated view of @fig-resultplot1. Siena/NYT (pink) shows relatively stable polling results for Harris, with fewer extreme values and a slight downward trend in October. On the other hand, Emerson (green) shows more fluctuation throughout the period with a wider spread of polling values. 

```{r}
#| label: fig-resultplot2
#| fig-cap: "Polling votes for Harris over time by pollster (*Note:* The date starts from after Harris officially announced her campaign for 2024 U.S. presidential election on July 21, 2024. Only filtered pollsters with high-quality polling questions are shown in the following.)"
#| echo: false
#| warning: false
#| message: false
#| fig-align: center
#| fig-width: 5
#| page-break-after: true


# Color by pollster
base_plot +
  geom_point(aes(color = pollster)) +
  geom_smooth() + 
  theme(legend.position = "bottom")
```

```{r}
#| label: fig-resultplot3
#| fig-cap: "Polling votes for Harris over time by pollster (facets) (*Note:* The date starts from after Harris officially announced her campaign for 2024 U.S. presidential election on July 21, 2024. Only filtered pollsters with high-quality polling questions are shown in the following.)"
#| echo: false
#| warning: false
#| fig-align: center
#| fig-width: 5
#| page-break-after: true

# Facet by pollster
base_plot +
  geom_point() +
  geom_smooth(method = "gam", formula = y ~ s(x)) +  # Use GAM smoothing to handle complex data
  facet_wrap(vars(pollster))
```

\newpage

When accounting for the state difference, @fig-resultplot4 and @fig-resultplot5 show variations in Harris' support levels. Critical battleground states like Pennsylvania and Michigan show distinct trends from the national analysis in @fig-resultplot1. Pennsylvania shows relatively stable trends slightly above the national average while Michigan shows fluctuations over time.

```{r}
#| label: fig-resultplot4
#| fig-cap: "Polling votes for Harris over time by state (*Note:* The date starts from after Harris officially announced her campaign for 2024 U.S. presidential election on July 21, 2024. Only filtered pollsters with high-quality polling questions are shown in the following.)"
#| echo: false
#| warning: false
#| message: false
#| fig-align: center
#| fig-width: 5
#| page-break-after: true

# Color by pollster
base_plot +
  geom_point(aes(color = state)) +
  geom_smooth() + 
  theme(legend.position = "bottom")
```

```{r}
#| label: fig-resultplot5
#| fig-cap: "Polling votes for Harris over time by state (facets) *Note:* The date starts from after Harris officially announced her campaign for 2024 U.S. presidential election on July 21, 2024. Only filtered pollsters with high-quality polling questions are shown in the following."
#| echo: false
#| warning: false
#| fig-align: center
#| fig-width: 5
#| page-break-after: true

# Facet by state
base_plot +
  geom_point() +
  geom_smooth(method = "gam", formula = y ~ s(x)) +  # Use GAM smoothing to handle complex data
  facet_wrap(vars(state))
```

\newpage

## Results from the prediction model


Prediction results derived from our model frameworks are summarized in @tbl-modelresults1 and @tbl-modelresults2. In the linear models (@tbl-modelresults1), higher $R^2$ and lower WAIC, RMSE values in the second column show that the inclusion of pollster effects improves accuracy and reliability of the prediction model. In the Bayesian models (@tbl-modelresults2), higher Log-Likelihood and lower ELPD, WAIC values in the fourth column shows that including both pollster and state effects enhances model performance.

\

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

model_date <-
  readRDS(file = here::here("models/model_date.rds"))

model_date_pollster <-
  readRDS(file = here::here("models/model_date_pollster.rds"))

bayesian_model_1 <-
  readRDS(file = here::here("models/bayesian_model_1.rds"))
  
bayesian_model_2 <-
  readRDS(file = here::here("models/bayesian_model_2.rds"))

```


```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults1
#| tbl-cap: "Linear models of support percentages for Harris based on date and pollster"
#| warning: false
#| tbl-width: 80% 
#| page-break-after: true
#| page-break-before: true

modelsummary(
  list(
    "Linear by Date" = model_date,
    "Linear by Date, Pollster" = model_date_pollster
  ),
  statistic = "mad",
  fmt = 2)

```

\newpage

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults2
#| tbl-cap: "Bayesian models of support percentages for Harris based on pollster and state"
#| warning: false
#| tbl-align: center
#| tbl-width: 80% 
#| page-break-after: true
#| page-break-before: true


modelsummary(
  list(
    "Bayesian with Pollster" = bayesian_model_1,
    "Bayesian with Pollster, State" = bayesian_model_2
  ),
  statistic = "mad",
  fmt = 2)

```


# Discussion {#sec-discussion}


## Why Harris could beat her polls {#sec-first-point}

@tbl-pctraw1 and @tbl-pctfilter1 shows that filtering data based on more reliable polling criteria makes the average rate of support for Harris increase. Given the current polling landscape where Harris and Trump have nearly equivalent levels of support [@factcheck2024], higher support rates for Harris in the filtered analysis serves as substantial evidence that Harris could win the election. 

Furthermore, our data analysis that incorporates state-specific factors (@fig-resultplot5) reveals that Harris holds relatively stable support in key battelground states, including Pennsylvania, Michigan, and Wisconsin. As it will be discussed in more detail in @sec-third-point, this suggests that Harris' support base is both resilient and potentially insulated from the unpredictable swings [@kff2024]. Regarding the winner-take-all nature of the electoral college, Harris' stability in these key states not only strengthens her position but also shows high probability in her victory, as even slight leads in battleground states can result in a decisive electoral advantage [@kff2024]. The combination of broad polling support and specific regional strength positions Harris as a leading presidential candidate in the 2024 election.

## Pollsters herding around false consensus {#sec-second-point}

@fig-resultplot2 and @fig-resultplot3 shows notable differences in the supportive trends for Harris when considering the variations of polling organizations. In @tbl-modelresults1, the inclusion of pollster-specific variables had statistical significance. This proposes that pollster-specific factors, like methodology and sample composition, produce a range of outcomes even for the same candidate's support levels [@nytimes2024]. As  pollsters often fear publishing outlier results that might undermine credibility, they may adjust the results with a perceived consensus [@silver2024]. This is likely to reduce the accuracy of polling aggregates, misrepresenting the true differences in public sentiment.

To be more specific, this adjustment based on previous consensus data would limit the visibility of demographic shifts that might favor one candidate unexpectedly [@aapor_herding]. For instance, the polling discrepancies in the 2016 and 2020 U.S election were partly attributed to underestimated certain voter segments supportive of Trump [@silver2024]. Assessing the potential limitations of aggregated data, recognizing that it may reflect methodological biases more than independent public opinion should be on the way for a reliable prediction of electoral support. 

## The electoral college and the power of battleground states {#sec-third-point}

In the U.S. electoral college, the Constitution assigns each state a set number of electoral based on its Congressional representation and a candidate must secure more than 270 votes to win the presidency [@whyy2024]. This often results in candidates focusing their campaigns primarily on battleground states, where voter preferences are more fluid and results are less predictable [@ashcenter2024]. @fig-resultplot5 show that key battleground states' polling trends differ from that of the national average (@fig-resultplot1). 

The stability of polling trends in Pennsylvania suggests a consistent voter base despite of the battleground status, which often means that even slight shifts in support could be pivotal [@ashcenter2024]. Harris' focus on healthcare reform, especially her commitment to strengthening the Affordable Care Act (ACA), is expected to have particular resonance in Pennsylvania, where a large population of low-to-middle income residents benefit from ACA subsidies [@factcheck2024]. On the other hand, Michigan, where there are economically diverse population, national economic policy discussions could swing voter sentiment more sharply and result in relatively large fluctuations [@kff2024]. Surveys carried out within battleground states show wider variation in candidate support, making the incorporation of state-specific factors into electoral models allow a better representation of the diverse political landscapes seen across the United States.

## Weaknesses and next steps

While our model provides a foundation to understand Harris' polling support, further refinements are necessary to enhance its accuracy and applicability. For instance, future studies could focus on the incorporation of voter demographics, such as age, gender, and education. Showing which segments of the population are driving changes in support, this can give further explanation to how certain policies or campaign events of candidates affect overall support for a candidate [@brown2024candidate]. In addition, interaction terms between pollster and state can be included in the model, considering that some states might receive disproportionately more attention from certain pollsters as shown in @fig-correlation3. This can give further explanation to how pollsters and regional dynamics affect overall support for a candidate [@pew2020electionpolls].

This study has its constraints in that sampling error and systematic biases inevitably occur in the process of measuring social phenomenon with data. In particular, due to the nature of polling surveys, non-responses or misunderstanding of the survey questions result in missing data or outliers [@pew2020electionpolls]. Considering the possibility of prediction errors, we should take caution in interpreting the analysis and prediction results. By conducting more in-depth research of the domain knowledge and constructing appropriate variables to put into account, it would help derive accurate conclusions rather than just acknowledging the numbers given itself [@geeksforgeeks2023domainknowledge]. 

\newpage

\appendix

# Appendix {#sec-appendix}

## Pollster methodology overview and evaluation

The New York Times/Siena College polling partnership, the polling organization that accounted for the majority of polls in our analysis (@fig-pollster), conducts polls tailored for specific elections, such as state or national races [@nyt_siena_2020_methodology]. Their sample size typically includes 600 to 1000 likely voters per poll, with oversampling in battleground states to capture regional nuances [@nyt_siena_2020_methodology]. The methodology uses random-digit dialing (RDD) for landlines and mobile phones to ensure representative sample coverage across demographics. In addition, online surveys are administered to complement phone-based responses, ensuring broader accessibility [@nyt_siena_2020_methodology]. The stratified random sampling approach is employed, where the population is divided into strata (based on demographic variables like race, education, and geography), and a random sample is drawn from each stratum [@tellingstories]. This allows for precision in reflecting the political leanings and key demographic shifts in specific regions.

The organization intends to enhance transparency in how public opinion is assessed, ensuring that questions are carefully designed to represent contemporary political discussions, and that the terminology is polished through a process of iterative testing to achieve clarity. They devote extensive resources to cognitive testing to ensure question wording reflects what the public thinks [@siena_scri_2024_poll]. Their polling methodology stands out in that its strategic focus on using representative samples reflect political leanings and demographics of a region for more contextual and precise polling. Siena/NYT has its reputation for accurately predicting key battleground state outcomes during previous elections, such as Florida in 2016[@siena_nyt_perfect_partnership_2024].

The limitations of Siena/NYT's methodology are the challenge of polling itself. Since polling is a "snapshot in time",  the results can fluctuate based on recent political events or campaign dynamics. Additionally, there are still issues with non-response bias in polling, particularly among the hard-to-reach voter or voters suspicious of polling organizations themselves [@pew2023polling].


## Idealized methodolgy

The proposed methodology for forecasting the 2024 U.S. presidential election with a budget of $100,000 would be designed as follows. First, a stratified random sampling method will be employed that allows for the capture of the demographic elements such as age, gender, race, and education level [@pew2023polling]. We will focus on 10 battleground states including Florida, Pennsylvania, Michigan, and Arizona, each receiving a budget of $7,000. For each state, $5,000 would be allocated for telephone and online surveys, while $2,000 would be allocated for training local staff for telephone outreach and conducting data entry to maintain data quality and consistency. This targeted investment could alleviate bias and make the sample more representative of the population [@pew2023polling], enhancing the reliability and inclusiveness of the gathered data.

In addition to robust sampling, the methodology incorporates high-quality questionnaire design and testing. We would invest $15,000 in this with $5,000 on initial drafting and expert review to ensure questions are clear and unbiased. $7,000 would be spend on conducting pre-tests and pilot studeies for refinement [@driveresearch_cognitive_testing] and $3,000 would be allocated to final review and translation of the survey questions. Cognitive testing and iterative feedback loops are expected to improve the validity and accuracy of the collected data to derive reliable prediction results [@presser2004pretesting].

Then, $10,000 would be assigned for data processing and weighting. Weighting methods that account for groups that are underrepresented ensure that the outcomes are not skewed by sampling errors [@pew2023polling]. Post-stratification weighting which ensures that the sample accurately represents the U.S voting population will take up $5,000 while data cleaning and consistency checks for preventing bias from erroneous entries or duplicates will take up $5,000.

The final model would use Bayesian hierarchical modeling, which allows for more flexible modeling of uncertainty and variation across states, pollsters, and other external factors. These models, along with out-of-sample testing and cross-validation, enable accurate prediction sensitive to the dynamics of real-world changes, including political events [@pew2023polling]. By investing $3,000 on model setup and calibration and $2,000 on cross-validation and testing, we expect to ensure the model remains robust and provides reliable predictions under various electoral scenarios. 

### Idealized survey

The proposed survey questionnarie design is in the following link:
\
https://forms.gle/zQ8iJyPk3HhJYrKd9.

::: {.layout}
### Survey demo

::: {.column width="1/5"}
![Survey intro](../other/idealized_survey/survey_intro.png)
:::

::: {.column width="1/5"}
![Survey questions asking about demographics](../other/idealized_survey/survey_q1.png)
:::

::: {.column width="1/5"}
![Survey questions asking for voting intentions and candidate favorability](../other/idealized_survey/survey_q2.png)
:::

::: {.column width="1/5"}
![Survey questions asking for issues of interest and likelihood to vote](../other/idealized_survey/survey_q3.png)
:::

::: {.column width="1/5"}
![Survey final thanks](../other/idealized_survey/survey_outro.png)
:::
:::


## Model details {#sec-model-details}

### Posterior predictive check

In the first posterior predictive check (@fig-ppcheck-1), we compare the observed data with replicated data generated from the posterior distribution. This shows that the model is able to replicate the overall distribution of the observed data, with the replicated curves (light blue) closely following the true data (dark blue line). This indicates that the model fits the data well in terms of capturing the main pattern or trend [@stan2023posteriorchecks].

In the second plot (@fig-ppcheck-2), the replicated data which had both the pollster and state variable as random intercepts, shows relatively closer approximation to the true data distribution. The narrowing of uncertainty in the posterior relative to the prior indicates the impact of the data on refining the model's predictions. This reassures that the model fits the data reasonably well and that the prior information has been appropriately updated by the observed data [@stan2023posteriorchecks].

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheck
#| layout-ncol: 2
#| fig-cap: "Examining how the Bayesian model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check"]
#| fig-align: center
#| fig-width: 5
#| page-break-after: true


pp_check(bayesian_model_1) +
  theme_classic() +
  theme(legend.position = "bottom")

pp_check(bayesian_model_2) +
  theme_classic() +
  theme(legend.position = "bottom")
```

### Diagnostics

@fig-stanareyouokay-1 is a trace plot. The sampled values for posterior distribution of intercept parameter across iterations of the MCMC algorithm shows good convergence [@stan_mcmc_traces]. The lines for the parameter appear to be stable and fluctuating around a central value without any clear trends or patterns. This suggests that the MCMC algorithm has likely converged, and the posterior samples are representative of the target distribution.

@fig-stanareyouokay-2 is a Rhat plot. The Rhat value is approximately 1.0 for the intercept, which shows that the variance within and between multiple chains have converged. An Rhat value close to 1 indicates that the chains have mixed well and are drawing from the same distribution while values significantly greater than 1 would indicate that further iterations are needed [@stan_mcmc_diagnostics].This suggests that the Bayesian models for both "pollster" and "state" have likely converged, and the results derived from these models are reliable.

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2
#| fig-align: center
#| fig-width: 5
#| page-break-after: true


plot(bayesian_model_1, pars = "(Intercept)", prob = 0.95)

plot(bayesian_model_2, pars = "(Intercept)", prob = 0.95)


```



\newpage


# References


